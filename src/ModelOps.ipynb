{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelOps\n",
    "The following steps incorporates saving the model within the S3 Bucket to have a model registry as an option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting truck_breakoff_rl_markov.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile truck_breakoff_rl_markov.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import pathlib\n",
    "from io import StringIO\n",
    "import argparse\n",
    "import joblib\n",
    "\n",
    "# saves model within s3 bucket\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def reinforcement_model(self):\n",
    "    # Define markov chain\n",
    "    # Define the transition matrix (Markov chain)\n",
    "    transition_matrix = np.array([[0.9, 0.1],\n",
    "                                [0.3, 0.7]])\n",
    "\n",
    "    # Define the reward matrix\n",
    "    reward_matrix = np.array([[10, -1],\n",
    "                            [-1, 10]])\n",
    "\n",
    "    # Define hyperparameters\n",
    "    num_episodes = 1000\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.95\n",
    "    epsilon = 0.1\n",
    "\n",
    "    # Define the Q-network\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_actions = transition_matrix.shape[1]\n",
    "    num_features = 4  # Number of features in your input data\n",
    "    W = tf.Variable(tf.random.uniform([num_states, num_actions], 0, 0.01))\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Initialize TensorFlow session\n",
    "    for episode in range(num_episodes):\n",
    "        state = np.random.randint(0, num_states)  # Start at a random state\n",
    "        while True:\n",
    "            # Choose action (epsilon-greedy)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(0, num_actions)\n",
    "            else:\n",
    "                one_hot_state = tf.reshape(tf.one_hot(state, num_states), [1, -1])\n",
    "                action = tf.argmax(tf.matmul(one_hot_state, W), 1).numpy()[0]\n",
    "            # Perform action and observe next state and reward\n",
    "            next_state = np.random.choice(range(num_states), p=transition_matrix[state])\n",
    "            hot_next_state = tf.reshape(tf.one_hot(next_state, num_states), [1, -1])\n",
    "            reward = reward_matrix[state, action]\n",
    "            # Compute Q-value of next state\n",
    "            Q_next = tf.matmul(hot_next_state, W)\n",
    "            # Update Q-value of current state\n",
    "            max_Q_next = tf.reduce_max(Q_next)\n",
    "            target_Q_values = tf.matmul(hot_next_state, W)\n",
    "            \n",
    "            # Update Q-value of current state\n",
    "            target_Q_values_updated = tf.identity(target_Q_values)  # Create a copy\n",
    "            target_Q_values_updated = tf.tensor_scatter_nd_update(target_Q_values_updated, [[0, action]], [reward + discount_factor * max_Q_next])\n",
    "\n",
    "            # Train Q-network\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = tf.matmul(one_hot_state, W)\n",
    "                loss = tf.reduce_sum(tf.square(target_Q_values_updated - Q_values))\n",
    "\n",
    "            gradients = tape.gradient(loss, [W])\n",
    "            optimizer.apply_gradients(zip(gradients, [W]))\n",
    "            state = next_state\n",
    "            if state == 0:  # Reached terminal state\n",
    "                break\n",
    "\n",
    "    # Save the learned model in model directory\n",
    "\n",
    "    tf.saved_model.save(W, '../model/truck_break_off_model')\n",
    "    # Print the learned Q-values\n",
    "    print(\"Learned Q-values:\")\n",
    "    print(W.numpy())\n",
    "\n",
    " def evaluate_model(self, model):\n",
    "    # Evaluate the model on the test set\n",
    "    num_states = self.transition_matrix.shape[0]\n",
    "    num_actions = self.transition_matrix.shape[1]\n",
    "    num_features = 7  # Number of features in your input data\n",
    "    correct_predictions = 0\n",
    "    for index, row in self.test.iterrows():\n",
    "        state = int(row['TRUCK_BREAK_OFF'])  # Convert state to integer\n",
    "        one_hot_state = tf.reshape(tf.one_hot(state, num_states), [1, -1])\n",
    "        action = tf.argmax(tf.matmul(one_hot_state, model), 1).numpy()[0]\n",
    "        # Assuming action 0 corresponds to no truck break off, action 1 corresponds to truck break off\n",
    "        predicted_break_off = action\n",
    "        true_break_off = row['LABEL']\n",
    "        if predicted_break_off == true_break_off:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    manual_calc_accuracy = correct_predictions / len(self.test)\n",
    "    print(\"Manual calculation accuracy:\", manual_calc_accuracy)\n",
    "\n",
    "    # Evaluate the model using sklearn metrics\n",
    "    y_true = self.test['LABEL']\n",
    "    y_pred = []\n",
    "    for index, row in self.test.iterrows():\n",
    "        state = int(row['ROUTEID'])  # Convert state to integer\n",
    "        one_hot_state = tf.reshape(tf.one_hot(state, num_states), [1, -1])\n",
    "        action = tf.argmax(tf.matmul(one_hot_state, model), 1).numpy()[0]\n",
    "        # Assuming action 0 corresponds to no truck break off, action 1 corresponds to truck break off\n",
    "        predicted_break_off = action\n",
    "        y_pred.append(predicted_break_off)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, zero_division=1)  # Set zero_division parameter\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)  # Set zero_division parameter\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", confusion)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[INFO] Extracting arguements...\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparamets sent by the cleinet are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--num_episodes\", type=int, default=1000)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--discount_factor\", type=float, default=0.95)\n",
    "    parser.add_argument(\"--epsilon\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--num_states\", type=int, default=transition_matrix.shape[0])\n",
    "    parser.add_argument(\"--num_actions\", type=int, default=transition_matrix.shape[1])\n",
    "    parser.add_argument(\"--num_features\", type=int, default=7)\n",
    "\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # sets the sagemaker environment within sagemaker\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAINING\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TESTING\"))\n",
    "\n",
    "    # test/train files\n",
    "    parser.add_argument(\"--train_file\", type=str, default=\"train-V1.csv\")\n",
    "    parser.add_argument(\"--test_file\", type=str, default=\"test-V1.csv\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"[INFO] Reading data...\")\n",
    "    print()\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "\n",
    "    print(\"[INFO] Building Training & Testing Datasets...\")\n",
    "    print()\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[label]\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[label]\n",
    "\n",
    "    print(\"[INFO] Training Model...\")\n",
    "    print()\n",
    "    model_dir = args.model_dir\n",
    "    reinforcement_model(model_dir)\n",
    "    model_fn(model_dir)\n",
    "    reinforcement_model(model_dir)\n",
    "\n",
    "    model_path = os.path.join(model_dir, \"model.joblib\")\n",
    "    joblib.dump(reinforcement_model, model_path)\n",
    "    print(\"Model saved at: {}\".format(model_path))\n",
    "    print()\n",
    "\n",
    "    print(\"[INFO] Model Training Complete...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role input & Sagemaker SKlearn\n",
    "Must get sagemaker role from IAM. In this particular instance we took an existing role for sagemaker (execution role) to enable this functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "# Create an SKLearn estimator\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"truck_breakoff_rl_markov.py\",\n",
    "    role=\"arn:aws:iam::174023208515:role/service-role/AmazonSageMaker-ExecutionRole-20240321T161040\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"truck-breakoff-rl-markov\",\n",
    "    hyperparameters={\n",
    "        \"num_episodes\": 1000,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"discount_factor\": 0.95,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"num_states\": 2,\n",
    "        \"num_actions\": 2,\n",
    "        \"num_features\": 4,\n",
    "    },\n",
    "    use_spot_instances=True,\n",
    "    max_wait = 7200,\n",
    "    max_run = 3600,\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"accuracy\", \"Regex\": \"Accuracy: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"precision\", \"Regex\": \"Precision: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"recall\", \"Regex\": \"Recall: ([0-9.]+).*$\"},\n",
    "        {\"Name\": \"f1\", \"Regex\": \"F1: ([0-9.]+).*$\"},\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: truck-breakoff-rl-markov-2024-04-04-05-31-41-957\n"
     ]
    }
   ],
   "source": [
    "# Launch training job with an async call\n",
    "train_path = \"s3://martymdlregistry/sagemaker/truck-break-off-rl_markov/train-V1.csv\"\n",
    "test_path = \"s3://martymdlregistry/sagemaker/truck-break-off-rl_markov/test-V1.csv\"\n",
    "sklearn_estimator.fit({\"train\": train_path, \"test\": test_path}, wait=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
